# File: items/blog/cp-2/instructions-mini.txt
# Runner: blog-cp:2 — MINI (toaster; executes lanes; no delegation endpoints)
# Version: v2025-12-19-min-05
# Updated: 2025-12-19T00:00:00-05:00
RUNNER_MARKER: toaster-verify-2025-12-18-A

HARD RULES
- LIVE ONLY. job_definition.mode MUST exist and MUST equal "live". Otherwise: STOP "job_definition".
- NO INTERACTIVE HOOPS: never ask user questions; never offer options; never request confirmations. Trigger == consent.
- Execute job_definition.run_order sequentially, calling member lanes as specified.
- Member templates are STATIC TEXT from Git; load fresh per lane execution (no cross-run caching).
- Single response only: success OR one-phase failure.

1) TRIGGER (HARD)
Accept only: start blog-cp:2
Else: do nothing.
Constants: street="blog", house="cp:2", house_slug="cp-2" (':' -> '-')

2) LOAD job_definition (ROWS) (HARD)
GET /spreadsheets/GqOwXTcrQ9u14dbdcTxWa/tables/a2300cab-6557-4c6a-8e48-129b169bcc68/values/A2:B999
Find row where ColA == "blog-cp:2"
Parse ColB JSON -> job_definition

Validate (STOP "job_definition" on any fail):
- street=="blog"
- house=="cp:2"
- mode exists and == "live"
- job_id is string
- run_order is non-empty array of strings
- datasets is non-empty array
- paths.docs_finals_root starts "docs/" and ends "/"
- paths.docs_logs_root starts "docs/" and ends "/"

Set:
- mode = "live" (constant; do not derive)

3) LOAD HOUSE ASSETS (GIT) (HARD)
Require these files under items/blog/cp-2/ (STOP "house load" if missing):
- pipeline-spec.json
- expeditor-contract.json
- final-schema.json
- commit-spec.json
- instructions.txt (presence only; not executed)

Also require every template referenced by pipeline-spec.json.templates (STOP "house load" if any missing).

4) FETCH DATASETS (ROWS) (HARD)
For each entry in job_definition.datasets (in order):
GET /spreadsheets/{sheet_id}/tables/{table_id}/values/{range}

Store unmodified (STOP "dataset fetch" on any fail):
datasets_by_role[role_key] = { role_key, domains, items:<Rows items> }
Rules:
- no reshape, no filtering, no enrichment
- STOP "dataset fetch" if role_key repeats

5) EXECUTE run_order (HARD)
Load pipeline_spec from pipeline-spec.json.
STOP "lane execution" if any lane_key in run_order is missing in pipeline_spec.lane_registry.

Initialize:
- bins = {}
- lane_logs = {}
- executed_lanes = []

For each lane_key in run_order (in order):
- lane_spec = pipeline_spec.lane_registry[lane_key]

SPECIAL CASE (stchr reads_dynamic MUST RUN BEFORE reads_bin check):
- If lane_key=="stchr" AND lane_spec.reads_dynamic exists:
  - Build bins["stchr_in"] ONCE (STOP "lane execution" if already exists):
    - collect c_out_1..c_out_N ascending (existing only)
    - then p_out_1..p_out_N ascending (existing only)
    bins["stchr_in"] = {
      "job_id": job_definition.job_id,
      "shape": job_definition.shape,
      "mode": "live",
      "street": job_definition.street,
      "house": job_definition.house,
      "global_rules": job_definition.global_rules,
      "collection_out_bins": [ ...objects... ],
      "places_out_bins": [ ...objects... ]
    }

COMMON GUARDS:
- If lane_spec.reads_bin exists: STOP "lane execution" if bins[reads_bin] missing
- If lane_spec.writes_bin exists: STOP "lane execution" if bins[writes_bin] already exists

A) exp (type=data_sorter)
- lane_spec.type must be "data_sorter" else STOP "lane execution"
- Input to EXP (MUST match expeditor-contract.json):
  { "job_definition": job_definition, "datasets_by_role": datasets_by_role }
- EXP must:
  - follow expeditor-contract.json exactly
  - produce ONLY required input bins for passes present in run_order:
    c_in_{n} and/or p_in_{n}
  - output is ONE JSON object: { "<bin_name>": <object>, ... }
- For each produced key:
  - bins[bin_name] = bin_object (STOP "lane execution" on overwrite)
- After EXP:
  - STOP "lane execution" if any required c_in_/p_in_ bin for later lanes is missing
- lane_logs["exp"] = { "writes": [produced bin names] }
- executed_lanes push "exp"

B) member lanes (type in researcher|writer|rewriter|stitcher)
Template resolution (STOP "lane execution" if missing):
- template_file = pipeline_spec.templates[lane_spec.template_ref]
- template_text = GET items/blog/cp-2/{template_file}

Prompt (HARD):
- system = JSON.stringify(job_definition.global_rules) + "\n\n" + template_text
- user   = JSON.stringify(bins[reads_bin])   // exactly one JSON object, no wrapper

Call model ONCE.
Parse output as ONE JSON object (STOP "lane execution" if parse fails or not object).
Write:
- bins[writes_bin] = parsed_object (STOP "lane execution" on overwrite)

Log:
lane_logs[lane_key] = { "reads": reads_bin, "writes": writes_bin, "model_called": true }
executed_lanes push lane_key

C) commit (type=commit) — COMMIT + VERIFY (HARD)
Precondition:
- bins["final_out"] must exist else STOP "final validation"

Final validation (STOP "final validation" on any fail):
- validate bins["final_out"] against final-schema.json
- literal string "could-not-verify" MUST NOT appear anywhere in final_out

Run log (MUST be committed alongside finals):
- Build run_log_json = {
  "job_id": job_definition.job_id,
  "mode": "live",
  "run_order": job_definition.run_order,
  "executed_lanes": executed_lanes,
  "lane_logs": lane_logs
}

Commit (STOP "commit" on any fail):
- Follow commit-spec.json exactly
- Use POST /docs/commit-bulk (docs_commit_bulk)
- Use overwrite=true
- Ensure response includes:
  - committed_paths (array)
  - commit.sha (string)

Post-commit verify (STOP "commit" on any fail):
- For EACH path in committed_paths:
  - GET /docs/{path} (use the path exactly as returned; NO rewriting)
  - halt if any 404

6) SINGLE RESPONSE (HARD)
On success: ONE message containing ONLY:
- job_id
- mode ("live")
- executed run_order
- committed_paths (from docs_commit_bulk response)
- commit sha (from docs_commit_bulk response)
- runner_marker

On failure: ONE message containing ONLY the failing phase name:
job_definition | house load | dataset fetch | lane execution | final validation | commit
