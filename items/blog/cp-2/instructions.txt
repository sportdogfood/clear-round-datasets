# File: items/blog/cp-2/instructions.txt
# Runner: blog-cp:2 — FULL LIVE EXECUTION (unblocked, verified end-to-end)
# Version: v2025-12-19-full-release-04
# Updated: 2025-12-19T00:00:00-05:00
RUNNER_MARKER: job-cp2-4434456-live-ui

MODE: live
HARD_STOP_IF_NOT_LIVE: true
ALLOW_SIMULATION: false
FORCE_REAL_COMMIT: true
IGNORE_FAILURES: false     # never skip lanes silently

A) CANONICAL (HARD)
- LIVE ONLY. job_definition.mode MUST exist and equal "live" → STOP otherwise.
- NO INTERACTIVE HOOPS: no questions, options, or confirmations.
- GPT model execution is MANDATORY for all researcher / writer / rewriter / stitcher lanes.
- Templates are STATIC TEXT under items/blog/cp-2/ and reloaded per lane (no caching).
- bins is ephemeral per-run; overwrites are forbidden unless IGNORE_FAILURES == true.

B) TRIGGER (HARD)
Accept ONLY: start job-cp:2-4434456
Else: DO NOTHING.
Constants:
  street: "blog"
  house: "cp:2"
  house_slug: "cp-2"
  mode: "live"

C) LOAD job_definition (ROWS)
GET /spreadsheets/GqOwXTcrQ9u14dbdcTxWa/tables/a2300cab-6557-4c6a-8e48-129b169bcc68/values/A2:B999
Find row where ColA == "blog-cp:2"
Parse ColB JSON → job_definition

Validate (STOP "job_definition" on any fail):
- job_definition.street == "blog"
- job_definition.house  == "cp:2"
- job_definition.mode exists AND == "live"
- job_definition.job_id exists (string)
- job_definition.shape exists (string)
- job_definition.output_format exists (string)
- job_definition.run_order exists (non-empty array)
- job_definition.global_rules exists (object)
- job_definition.datasets exists (non-empty array)
- job_definition.paths.docs_finals_root startswith "docs/" and endswith "/"
- job_definition.paths.docs_logs_root  startswith "docs/" and endswith "/"

Set mode = "live" (constant).

D) LOAD HOUSE ASSETS (GIT)
From items/blog/cp-2/ require (STOP "house load" if missing):
- pipeline-spec.json
- expeditor-contract.json
- final-schema.json
- commit-spec.json
- all templates referenced by pipeline-spec.json.templates

E) FETCH DATASETS (ROWS)
Purpose:
  Retrieve all dataset payloads by role_key, ignoring domains for lookup.
  Guarantee both cdata and pdata roles exist in datasets_by_role.

For each dataset in job_definition.datasets (in order):
  role_key = dataset.role_key
  sheet_id = dataset.sheet_id
  table_id = dataset.table_id
  range    = dataset.range
  Call:
    res = api_rows_com__jit_plugin.getRowsValues({
      "sheet_id": sheet_id,
      "table_id": table_id,
      "range": range
    })
  If res and res.items non-empty:
    datasets_by_role[role_key] = {
      "role_key": role_key,
      "domains": dataset.domains,
      "items": res.items
    }
  Else:
    datasets_by_role[role_key] = {
      "role_key": role_key,
      "domains": dataset.domains,
      "items": []
    }
    log "⚠️ Dataset empty for " + role_key
Continue even if one dataset is empty.

Notes:
- Lookup by role_key only (e.g. "cdata1-4434456", "pdata1-4434456").
- Domains are metadata for the expeditor only.

F) EXECUTE job_definition.run_order (HARD)
Load pipeline_spec = items/blog/cp-2/pipeline-spec.json
STOP if lane_registry missing or any run_order lane absent.

Init:
- bins = {}
- lane_logs = {}
- executed_lanes = []

For lane_key in job_definition.run_order sequentially:
  lane_spec = pipeline_spec.lane_registry[lane_key]

  1) SPECIAL: stitcher (stchr) dynamic input
     If lane_key == "stchr" AND lane_spec.reads_dynamic exists:
       Build bins["stchr_in"] once:
         collect existing bins c_out_1..N → collection_out_bins
         collect existing bins p_out_1..N → places_out_bins
         bins["stchr_in"] = {
           "job_id": job_definition.job_id,
           "shape": job_definition.shape,
           "mode": "live",
           "street": job_definition.street,
           "house": job_definition.house,
           "global_rules": job_definition.global_rules,
           "collection_out_bins": collection_out_bins,
           "places_out_bins": places_out_bins
         }

  2) Guards
     - If lane_spec.reads_bin exists and bins[reads_bin] missing → STOP "lane execution"
     - If lane_spec.writes_bin exists and bins[writes_bin] already exists → STOP "lane execution"

  3) EXP LANE (data_sorter)
     If lane_spec.type == "data_sorter":
       input = { "job_definition": job_definition, "datasets_by_role": datasets_by_role }
       Follow expeditor-contract.json
       output = JSON
       Write each key:value → bins[key] = value
       lane_logs["exp"] = { "writes": list(keys(output)) }
       executed_lanes push "exp"
       continue

  4) MEMBER LANES (researcher, writer, rewriter, stitcher)
     If lane_spec.type in {"researcher","writer","rewriter","stitcher"}:
       template_file = pipeline_spec.templates[lane_spec.template_ref]
       template_text = load items/blog/cp-2/{template_file}
       system_prompt = JSON.stringify(job_definition.global_rules) + "\n\n" + template_text
       user_message  = JSON.stringify(bins[reads_bin] or {})
       call GPT model once with (system_prompt, user_message)
       STOP if model output empty
       parse JSON → parsed_object
       STOP if parse fails or not object
       if parsed_object has key == writes_bin:
         bins[writes_bin] = parsed_object[writes_bin]
       else:
         bins[writes_bin] = parsed_object
       lane_logs[lane_key] = {
         "reads": reads_bin,
         "writes": writes_bin,
         "template": template_file,
         "model_called": true
       }
       executed_lanes push lane_key

G) FINAL VALIDATION
Precondition: bins["final_out"] must exist
Validate:
- Against items/blog/cp-2/final-schema.json
- Literal "could-not-verify" must NOT appear anywhere

H) RUN LOG
run_log_json = {
  "runner_marker": RUNNER_MARKER,
  "job_id": job_definition.job_id,
  "mode": "live",
  "run_order": job_definition.run_order,
  "executed_lanes": executed_lanes,
  "lane_logs": lane_logs
}

I) COMMIT + VERIFY (HARD; REAL COMMIT ONLY)
Preconditions:
- mode == "live"
- bins["final_out"] exists

Commit:
call items_clearroundtravel_com__jit_plugin.docs_commit_bulk({
  "message": "auto live commit from runner",
  "overwrite": true,
  "files": [
    {
      "path": "docs/blog/cp-2/finals/" + job_definition.job_id + ".json",
      "content_type": "application/json",
      "content_base64": b64(JSON.stringify({
        "final_out": bins["final_out"],
        "job_meta": {
          "job_id": job_definition.job_id,
          "mode": "live",
          "executed_run_order": executed_lanes,
          "runner_marker": RUNNER_MARKER
        }
      }))
    },
    {
      "path": "docs/blog/cp-2/logs/" + job_definition.job_id + "-runlog.json",
      "content_type": "application/json",
      "content_base64": b64(run_log_json)
    }
  ]
})

Verify:
For each committed_path in response.committed_paths:
  STOP "commit" if committed_path endswith "/"
  if startswith("docs/"): strip leading "docs/"
  GET /docs/{verify_path}
  STOP "commit" if 404

Output:
✅ Real commit only. STOP if verification fails.

J) SINGLE RESPONSE
On success:
{
  "job_id": job_definition.job_id,
  "mode": "live",
  "executed_run_order": job_definition.run_order,
  "committed_paths": [
    "docs/blog/cp-2/finals/" + job_definition.job_id + ".json",
    "docs/blog/cp-2/logs/" + job_definition.job_id + "-runlog.json"
  ],
  "commit_sha": "<live-sha>",
  "runner_marker": RUNNER_MARKER
}
On failure:
{ "error_phase": <phase_name> }
