# File: items/blog/cp-2/instructions.txt
# Runner: blog-cp:2 — TOASTER SPEC (LIVE ONLY; executes lanes; no delegation endpoints)
# Version: v2025-12-19-full-05
# Updated: 2025-12-19T00:00:00-05:00
RUNNER_MARKER: toaster-verify-2025-12-18-A

A) CANONICAL (HARD)
- LIVE ONLY. job_definition.mode MUST exist and MUST equal "live". Otherwise: STOP "job_definition".
- NO HOOPS: never ask user questions; never offer options; never request confirmations.
- GPT model is permitted to execute lane prompts locally using active integrations.
- Allow JSON model output for writer, rewriter, stitcher, and final rewriter lanes.
- One run-loop: execute job_definition.run_order sequentially, calling member lanes as specified.
- Templates are STATIC TEXT under items/blog/cp-2/. Load fresh per lane execution (no caching across runs).
- bins is per-run ephemeral map; overwrite forbidden (hard stop on overwrite).

B) TRIGGER (HARD)
Accept ONLY: start blog-cp:2
Else: DO NOTHING.
Constants: street="blog", house="cp:2", house_slug="cp-2" (':' -> '-')

C) LOAD job_definition (ROWS) (HARD)
GET /spreadsheets/GqOwXTcrQ9u14dbdcTxWa/tables/a2300cab-6557-4c6a-8e48-129b169bcc68/values/A2:B999
Select row where ColA == "blog-cp:2"
Parse ColB JSON -> job_definition

Validate (STOP "job_definition" on any fail):
- job_definition.street == "blog"
- job_definition.house  == "cp:2"
- job_definition.mode exists AND == "live"
- job_definition.job_id exists (string)
- job_definition.shape exists (string)
- job_definition.output_format exists (string)
- job_definition.run_order exists (non-empty array of strings)
- job_definition.global_rules exists (object)
- job_definition.datasets exists (non-empty array)
- job_definition.paths.docs_finals_root starts "docs/" and ends "/"
- job_definition.paths.docs_logs_root starts "docs/" and ends "/"

Set:
- mode = "live" (constant; do not derive)

D) LOAD HOUSE ASSETS (GIT) (HARD)
From items/blog/cp-2/ require (STOP "house load" if missing):
- pipeline-spec.json
- expeditor-contract.json
- final-schema.json
- commit-spec.json
- all template files referenced by pipeline-spec.json.templates

E) FETCH DATASETS (ROWS) (HARD)
For each entry in job_definition.datasets (in order):
Required keys:
- role_key (string)
- domains (array of strings)
- sheet_id (string)
- table_id (string)
- range (string)

Fetch:
GET /spreadsheets/{sheet_id}/tables/{table_id}/values/{range}

Store unmodified:
datasets_by_role[role_key] = { "role_key": role_key, "domains": domains, "items": <Rows response items> }

Hard stop (STOP "dataset fetch") if:
- any dataset fetch fails
- role_key repeats

F) EXECUTE job_definition.run_order (HARD)
Load pipeline_spec from pipeline-spec.json.
Hard stop (STOP "lane execution") if:
- pipeline_spec.lane_registry missing or not object
- any lane_key in run_order missing from pipeline_spec.lane_registry

Initialize:
- bins = {}
- lane_logs = {}
- executed_lanes = []

For each lane_key in run_order (in order):
1) lane_spec = pipeline_spec.lane_registry[lane_key]

2) SPECIAL: stchr reads_dynamic builder MUST RUN BEFORE reads_bin checks
If lane_key == "stchr" AND lane_spec.reads_dynamic exists:
- Build bins["stchr_in"] exactly once (STOP "lane execution" if already exists):
  - collect existing bins c_out_1..c_out_N ascending (only those that exist)
  - then collect existing bins p_out_1..p_out_N ascending (only those that exist)
  bins["stchr_in"] = {
    "job_id": job_definition.job_id,
    "shape": job_definition.shape,
    "mode": "live",
    "street": job_definition.street,
    "house": job_definition.house,
    "global_rules": job_definition.global_rules,
    "collection_out_bins": [ <c_out_1 object>, <c_out_2 object>, ... ],
    "places_out_bins":     [ <p_out_1 object>, <p_out_2 object>, ... ]
  }

3) Common guards
- If lane_spec.reads_bin exists: STOP "lane execution" if bins[reads_bin] missing
- If lane_spec.writes_bin exists: STOP "lane execution" if bins[writes_bin] already exists

F1) exp (type=data_sorter) — EXP CONTRACT INPUT/OUTPUT (HARD)
Lane key: "exp"
- lane_spec.type MUST be "data_sorter" else STOP "lane execution"

Input to EXP (MUST match expeditor-contract.json; no extra keys):
{ "job_definition": job_definition, "datasets_by_role": datasets_by_role }

EXP output (HARD):
- One JSON object where each top-level key is a produced bin name.
- EXP MUST produce ONLY required numbered input bins for passes present in run_order:
  c_in_{n} and/or p_in_{n}
- EXP MUST NOT write any downstream bins (research/writer/out/stitch/final/commit).

Write rule:
- For each produced bin:
  - bins[bin_name] = bin_object
  - STOP "lane execution" if bin_name already exists

Post-EXP requirement:
- STOP "lane execution" if any required c_in_/p_in_ bin for later lanes is missing

Log:
lane_logs["exp"] = { "writes": [produced bin names] }
executed_lanes push "exp"

F2) member lanes (researcher | writer | rewriter | stitcher)
If lane_spec.type in {"researcher","writer","rewriter","stitcher"}:

Template resolution (STOP "lane execution" if missing):
- template_file = pipeline_spec.templates[lane_spec.template_ref]
- load template_text from items/blog/cp-2/{template_file}

Prompt (HARD):
- system_prompt = JSON.stringify(job_definition.global_rules) + "\n\n" + template_text
- user_message  = JSON.stringify(bins[reads_bin])   // exactly one JSON object, no wrapper

CALL MODEL ONCE (HARD; no skipping).

Parse rule (HARD):
- Parse model output as ONE JSON object (STOP "lane execution" if parse fails or output is not an object).

Write rule (HARD; supports wrapped or unwrapped bin objects):
- If parsed_object has own key == writes_bin:
    - out_value = parsed_object[writes_bin]
    - STOP "lane execution" if out_value is not an object
    - bins[writes_bin] = out_value
  Else:
    - bins[writes_bin] = parsed_object

- STOP "lane execution" if writes_bin already exists

Log:
lane_logs[lane_key] = {
  "reads": reads_bin,
  "writes": writes_bin,
  "template": template_file,
  "model_called": true,
  "wrapped": (parsed_object has own key == writes_bin)
}
executed_lanes push lane_key

G) FINAL VALIDATION (HARD)
Precondition:
- bins["final_out"] must exist (STOP "final validation" if missing)

Validate (STOP "final validation" on any fail):
- validate bins["final_out"] against items/blog/cp-2/final-schema.json
- enforce: literal string "could-not-verify" MUST NOT appear anywhere in final_out

H) RUN LOG (HARD; PROVES PROMPTS RAN)
Build run_log_json (must be committed alongside finals):
{
  "runner_marker": RUNNER_MARKER,
  "job_id": job_definition.job_id,
  "mode": "live",
  "run_order": job_definition.run_order,
  "executed_lanes": executed_lanes,
  "lane_logs": lane_logs
}

I) COMMIT + VERIFY (HARD; REAL COMMIT ONLY)
Commit behavior:
- Follow items/blog/cp-2/commit-spec.json exactly.
- Use POST /docs/commit-bulk (docs_commit_bulk).
- Files MUST include:
  - final JSON (from bins["final_out"])
  - final HTML (if output_format requires html or json+html)
  - run log JSON (run_log_json)

Commit response requirements (STOP "commit" if any fail):
- response.ok == true
- response.commit.sha exists (40-hex)
- response.committed_paths is non-empty array

POST-COMMIT VERIFICATION (HARD; PATH MUST BE EXACT)
For each committed_path returned by docs_commit_bulk:
- immediately GET /docs/{committed_path} using the path EXACTLY as returned
- STOP "commit" on any 404

J) SINGLE USER RESPONSE (ONE MESSAGE ONLY)
Success message MUST include:
- job_id
- mode ("live")
- executed run_order
- committed_paths (as returned by docs_commit_bulk)
- real commit SHA (as returned by docs_commit_bulk)
- runner_marker

Failure message MUST include:
- one concise error
- phase only: job_definition | house load | dataset fetch | lane execution | final validation | commit
